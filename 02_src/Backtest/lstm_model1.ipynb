{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lstm_model1.ipynb","provenance":[],"authorship_tag":"ABX9TyPCuPMYMcuyndjlbS9vCGwn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ClZDtlBqjup","executionInfo":{"status":"ok","timestamp":1660771501273,"user_tz":420,"elapsed":27093,"user":{"displayName":"Wei Dongyu","userId":"05870905830267306106"}},"outputId":"e2f9a44c-05b8-4f51-e5bc-7e06eb53d09c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"]},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ray\n","  Downloading ray-1.13.0-cp37-cp37m-manylinux2014_x86_64.whl (54.5 MB)\n","\u001b[K     |████████████████████████████████| 54.5 MB 115 kB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n","Requirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray) (1.3.1)\n","Collecting virtualenv\n","  Downloading virtualenv-20.16.3-py2.py3-none-any.whl (8.8 MB)\n","\u001b[K     |████████████████████████████████| 8.8 MB 35.9 MB/s \n","\u001b[?25hRequirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray) (1.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray) (2.23.0)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.21.6)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.4)\n","Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n","Collecting grpcio<=1.43.0,>=1.28.1\n","  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n","\u001b[K     |████████████████████████████████| 4.1 MB 47.5 MB/s \n","\u001b[?25hRequirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (22.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.8.0)\n","Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray) (1.15.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.12.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.9.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.1.1)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (1.24.3)\n","Collecting platformdirs<3,>=2.4\n","  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n","Collecting distlib<1,>=0.3.5\n","  Downloading distlib-0.3.5-py2.py3-none-any.whl (466 kB)\n","\u001b[K     |████████████████████████████████| 466 kB 56.4 MB/s \n","\u001b[?25hInstalling collected packages: platformdirs, distlib, virtualenv, grpcio, ray\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.47.0\n","    Uninstalling grpcio-1.47.0:\n","      Successfully uninstalled grpcio-1.47.0\n","Successfully installed distlib-0.3.5 grpcio-1.43.0 platformdirs-2.5.2 ray-1.13.0 virtualenv-20.16.3\n"]}],"source":["import os\n","import time\n","import pandas as pd\n","import numpy as np\n","import warnings\n","import statsmodels.api as sm\n","from tqdm import tqdm\n","import seaborn as sns\n","import datetime\n","import time\n","import matplotlib.pyplot as plt\n","import statsmodels.stats as stat\n","import sys\n","import scipy.stats as stat\n","import sympy as sy\n","from sympy.stats import Normal, cdf\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import StratifiedKFold,KFold\n","import sklearn\n","import joblib\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import lightgbm as lgb\n","import sklearn as sk\n","!pip install ray\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import ASHAScheduler\n","#import pickle5 as pickle\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","tqdm.pandas(desc=\"my bars:\")\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4xMc0cEqt7N","executionInfo":{"status":"ok","timestamp":1660771521239,"user_tz":420,"elapsed":15063,"user":{"displayName":"Wei Dongyu","userId":"05870905830267306106"}},"outputId":"3c5ae92b-008e-43c1-89aa-06f0b78b8e83"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["feature_train = pd.read_csv(\"/content/drive/MyDrive/Huatai_ml_model/feature_train.csv\",index_col=0)\n","feature_test = pd.read_csv(\"/content/drive/MyDrive/Huatai_ml_model/feature_test.csv\",index_col=0)\n","X_train = feature_train.drop(['time','code','option_code','type','maturity_date','f_close_1min','f_ret_1min','f_ret_direction'],axis=1)\n","y_train = feature_train['f_ret_direction']\n","X_test = feature_test.drop(['time','code','option_code','type','maturity_date','f_close_1min','f_ret_1min','f_ret_direction'],axis=1)\n","y_test = feature_test['f_ret_direction']\n","#lstm dataset\n","stock_feature_train_list = []\n","stock_feature_test_list = []\n","code_list = feature_train.code.unique()\n","for code in code_list:\n","    stock_feature_train_list.append(feature_train[feature_train.code==code])\n","    stock_feature_test_list.append(feature_test[feature_test.code==code])"],"metadata":{"id":"LssenjmRqw5K","executionInfo":{"status":"ok","timestamp":1660771524827,"user_tz":420,"elapsed":1069,"user":{"displayName":"Wei Dongyu","userId":"05870905830267306106"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def batch_data_num(feature_list,sequence_len,batchsize):\n","    #return dataloader_list\n","    dataloader_list = []\n","    for features in feature_list:\n","        feature_tensors = []\n","        target_tensors = []\n","        if len(features)<sequence_len:\n","            continue\n","        for idx in range(0,len(features) - sequence_len):\n","\n","            x_train = features.drop(['time','code','option_code','type','maturity_date','f_close_1min','f_ret_direction'],axis=1)\n","            y_train = features['f_close_1min']\n","            feature_tensors.append(x_train.iloc[idx:idx+sequence_len].values)\n","            target_tensors.append(y_train.iloc[idx])\n","        feature_tensors = torch.from_numpy(np.array(feature_tensors))\n","        target_tensors = torch.from_numpy(np.array(target_tensors))\n","        data = TensorDataset(feature_tensors,target_tensors)\n","        data_loader = torch.utils.data.DataLoader(data,shuffle=False,batch_size = batchsize)\n","        dataloader_list.append(data_loader)\n","    return dataloader_list\n","\n","def batch_data_1(feature_list,sequence_len,batchsize,y_name):\n","    #return one dataloader\n","    feature_tensors = []\n","    target_tensors = []\n","    for features in feature_list:\n","        if len(features)<sequence_len:\n","            continue\n","        for idx in range(0,len(features) - sequence_len):\n","            x_train = features.drop(['time','code','option_code','type','maturity_date','f_close_1min','f_ret_1min','f_ret_direction'],axis=1)\n","            y_train = features[y_name]\n","            feature_tensors.append(x_train.iloc[idx:idx+sequence_len].values)\n","            target_tensors.append(y_train.iloc[idx])\n","    feature_tensors = torch.from_numpy(np.array(feature_tensors))\n","    target_tensors = torch.from_numpy(np.array(target_tensors))\n","    data = TensorDataset(feature_tensors,target_tensors)\n","    del feature_tensors\n","    del target_tensors\n","    data_loader = torch.utils.data.DataLoader(data,shuffle=False,batch_size = batchsize)\n","    return data_loader"],"metadata":{"id":"MhzxYiEDqyre","executionInfo":{"status":"ok","timestamp":1660771527333,"user_tz":420,"elapsed":205,"user":{"displayName":"Wei Dongyu","userId":"05870905830267306106"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["stock_train_dataloader = batch_data_1(stock_feature_train_list,240,32,'f_ret_direction')\n","stock_test_dataloader = batch_data_1(stock_feature_test_list,240,32,'f_ret_direction')"],"metadata":{"id":"kg4WEaSGrZz8","executionInfo":{"status":"ok","timestamp":1660771539792,"user_tz":420,"elapsed":10678,"user":{"displayName":"Wei Dongyu","userId":"05870905830267306106"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class LSTM(nn.Module):\n","    \n","    def __init__(self, num_features,hidden_dim, n_layers, dropout=0.5,sequence_length=240):\n","        super(LSTM, self).__init__()\n","        # set class variables\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","        self.num_features = num_features\n","        self.sequence_length = sequence_length\n","        # define model layers\n","        self.lstm = nn.LSTM(num_features,hidden_dim,n_layers,dropout = dropout, batch_first = True)\n","        self.fc = nn.Linear(hidden_dim*sequence_length,1)\n","        self.dropout = nn.Dropout(dropout)\n","    def forward(self,x,hidden):\n","        x,hidden = self.lstm(x,hidden)\n","        #x = x.contiguous().view(-1, self.hidden_dim)\n","        x = x.contiguous().view(x.shape[0],-1)\n","        x = self.dropout(x)\n","        x = F.sigmoid(self.fc(x))\n","        x = x.squeeze()\n","        return x,hidden\n","    def init_hidden(self,batch_size):\n","        train_on_gpu=torch.cuda.is_available()\n","        weight = next(self.parameters()).data\n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n","        return hidden"],"metadata":{"id":"9evePCC2rozY","executionInfo":{"status":"ok","timestamp":1660771560309,"user_tz":420,"elapsed":162,"user":{"displayName":"Wei Dongyu","userId":"05870905830267306106"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden,clip=5):\n","    \n","    # move data to GPU, if available\n","    \n","    train_on_gpu=torch.cuda.is_available()\n","    if train_on_gpu:\n","        inp,target,rnn = inp.cuda(),target.cuda(),rnn.cuda()\n","    h = tuple([each.data for each in hidden])\n","    optimizer.zero_grad()\n","    \n","    output,h = rnn(inp,h)\n","    loss = criterion(output,target)\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n","    optimizer.step()\n","    \n","    return loss.item(),h"],"metadata":{"id":"cUuJb99NrpZH","executionInfo":{"status":"ok","timestamp":1660771561251,"user_tz":420,"elapsed":146,"user":{"displayName":"Wei Dongyu","userId":"05870905830267306106"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n","    batch_losses = []\n","    \n","    rnn.train()\n","\n","    print(\"Training for %d epoch(s)...\" % n_epochs)\n","    for epoch_i in range(1, n_epochs + 1):\n","        \n","        # initialize hidden state\n","        hidden = rnn.init_hidden(batch_size)\n","        \n","        for batch_i, (inputs, labels) in enumerate(stock_train_dataloader, 1):\n","            labels = labels.double()\n","            # make sure you iterate over completely full batches, only\n","            n_batches = len(stock_train_dataloader.dataset)//batch_size\n","            if(batch_i > n_batches):\n","                break\n","            \n","            # forward, back prop\n","            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n","            # record loss\n","            batch_losses.append(loss)\n","\n","            # printing loss stats\n","            if batch_i % show_every_n_batches == 0:\n","                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n","                    epoch_i, n_epochs, np.average(batch_losses)))\n","                batch_losses = []\n","\n","    # returns a trained rnn\n","    return rnn"],"metadata":{"id":"t3zg5-FHrrCZ","executionInfo":{"status":"ok","timestamp":1660771562294,"user_tz":420,"elapsed":244,"user":{"displayName":"Wei Dongyu","userId":"05870905830267306106"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["epoches = 10\n","num_features = 31\n","hidden_dim = 64\n","n_layers = 5\n","learning_rate = 0.00001\n","lstm = LSTM(num_features=num_features,hidden_dim=hidden_dim, n_layers = n_layers, dropout=0.5).double()\n","train_on_gpu = torch.cuda.is_available()\n","if train_on_gpu:\n","    lstm.to(\"cuda\")\n","\n","# defining loss and optimization functions for training\n","optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n","criterion = nn.BCEWithLogitsLoss()\n","trained_lstm = train_rnn(lstm, 32, optimizer, criterion, epoches)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2L--uH62rt-c","executionInfo":{"status":"ok","timestamp":1660773292845,"user_tz":420,"elapsed":1729169,"user":{"displayName":"Wei Dongyu","userId":"05870905830267306106"}},"outputId":"2eea6fd8-ff5f-458b-c91d-04a0e6635e49"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Training for 10 epoch(s)...\n","Epoch:    1/10    Loss: 0.7136464314017259\n","\n","Epoch:    1/10    Loss: 0.7150333337786173\n","\n","Epoch:    1/10    Loss: 0.7089091453467894\n","\n","Epoch:    2/10    Loss: 0.6965989980077001\n","\n","Epoch:    2/10    Loss: 0.6957139544328319\n","\n","Epoch:    2/10    Loss: 0.696077790848691\n","\n","Epoch:    3/10    Loss: 0.6938060506870357\n","\n","Epoch:    3/10    Loss: 0.6937593285556867\n","\n","Epoch:    3/10    Loss: 0.6940169494219818\n","\n","Epoch:    4/10    Loss: 0.6933448649939673\n","\n","Epoch:    4/10    Loss: 0.6933349461129559\n","\n","Epoch:    4/10    Loss: 0.6934573360309767\n","\n","Epoch:    5/10    Loss: 0.6932272388984888\n","\n","Epoch:    5/10    Loss: 0.6932199568105081\n","\n","Epoch:    5/10    Loss: 0.6932780210473204\n","\n","Epoch:    6/10    Loss: 0.6931823815910386\n","\n","Epoch:    6/10    Loss: 0.6931780837405725\n","\n","Epoch:    6/10    Loss: 0.6932051997025134\n","\n","Epoch:    7/10    Loss: 0.6931647070508288\n","\n","Epoch:    7/10    Loss: 0.6931609532852667\n","\n","Epoch:    7/10    Loss: 0.6931770128653341\n","\n","Epoch:    8/10    Loss: 0.6931565841441549\n","\n","Epoch:    8/10    Loss: 0.6931545840929024\n","\n","Epoch:    8/10    Loss: 0.6931638458171165\n","\n","Epoch:    9/10    Loss: 0.6931526414685998\n","\n","Epoch:    9/10    Loss: 0.6931513864286559\n","\n","Epoch:    9/10    Loss: 0.6931571929408679\n","\n","Epoch:   10/10    Loss: 0.6931503230385009\n","\n","Epoch:   10/10    Loss: 0.6931494631048097\n","\n","Epoch:   10/10    Loss: 0.693153687866225\n","\n"]}]},{"cell_type":"code","source":["test_loss = 0\n","trained_lstm.eval()\n","hidden = trained_lstm.init_hidden(32)\n","for inputs,labels in stock_train_dataloader:\n","    train_on_gpu=torch.cuda.is_available()\n","    h = tuple([each.data for each in hidden])\n","    if train_on_gpu:\n","        inputs,trained_lstm,labels = inputs.cuda(),trained_lstm.cuda(),labels.cuda()\n","    output,h = trained_lstm(inputs,h)\n","    loss = criterion(output,labels)\n","    test_loss+=loss.item()\n","test_loss = test_loss/len(stock_test_dataloader.dataset)\n","print(test_loss)"],"metadata":{"id":"vUq8VgB7rv_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stock_train_dataloader_dict = {}\n","stock_test_dataloader_dict = {}\n","for batch_size in [32,64,128]:\n","  stock_train_dataloader_dict[batch_size] = batch_data_1(stock_feature_train_list,240,batch_size,'f_ret_1min')\n","  stock_test_dataloader_dict[batch_size] = batch_data_1(stock_feature_test_list,240,batch_size,'f_ret_1min')"],"metadata":{"id":"V_G_Th9RrwX8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def grid_train_rnn(config,epoches = 20,checkpoint_dir = None):\n","    lstm_sabr = LSTM(31,config[\"hidden\"],config[\"n_layer\"],config[\"dropout\"])\n","    lstm_sabr = lstm_sabr.double()\n","    train_on_gpu = torch.cuda.is_available()\n","    if train_on_gpu:\n","        lstm_sabr.cuda()\n","    optimizer = torch.optim.Adam(lstm.parameters(), lr=config[\"lr\"])\n","    criterion = nn.MSELoss()\n","    train_loader = stock_train_dataloader_dict[config[\"batch_size\"]]\n","    \n","    batch_losses = []  \n","    lstm_sabr.train()\n","\n","    for epoch_i in range(1, epoches + 1):\n","        running_loss = 0\n","        # initialize hidden state\n","        hidden = lstm_sabr.init_hidden(config[\"batch_size\"])\n","        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n","        # make sure you iterate over completely full batches, only\n","            n_batches = len(stock_train_dataloader.dataset)//batch_size\n","            if(batch_i > n_batches):\n","                break\n","            \n","          # forward, back prop\n","            loss, hidden = forward_back_prop(lstm_sabr, optimizer, criterion, inputs, labels, hidden)          \n","          # record loss\n","            batch_losses.append(loss)\n","            running_loss+=loss.item()\n","\n","        with tune.checkpoint_dir(epoch_i) as checkpoint_dir:\n","            path = os.path.join(checkpoint_dir,\"checkpoint\")\n","            torch.save((lstm_sabr.state_dict(),optimizer.state_dict()),path)\n","        tune.report(loss = running_loss/len(train_loader.dataset))\n","    print(\"Finish Training\")\n"],"metadata":{"id":"8g6OR4C_ryDn"},"execution_count":null,"outputs":[]}]}